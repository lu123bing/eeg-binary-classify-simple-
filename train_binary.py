import hydra
from omegaconf import DictConfig
import torch
from models import create_binary_model
from pl_models import BinaryExtractorModel
import numpy as np
import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger
import wandb
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pl_datamodule import EEGBinaryDataModule
import os
import logging

log = logging.getLogger(__name__)

@hydra.main(config_path="../cfgs", config_name="binary_config", version_base="1.3")
def train_binary_ext(cfg: DictConfig) -> None:
    """è®­ç»ƒäºŒåˆ†ç±»æå–å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    print("ðŸš€ å¼€å§‹è®­ç»ƒEEGäºŒåˆ†ç±»æå–å™¨ (ä¼˜åŒ–ç‰ˆ)")
    print(f"ðŸ” æ¨¡åž‹: {cfg.model.model_type}")
    
    # ðŸ”¥ ç³»ç»Ÿä¼˜åŒ–è®¾ç½®
    if torch.cuda.is_available():
        # print(f"ðŸ” ç³»ç»Ÿæ£€æŸ¥:")
        # print(f"   CUDAå¯ç”¨: True")
        # print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
        # print(f"   GPU: {torch.cuda.get_device_name(0)}")
        # print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # ðŸ”¥ CUDAä¼˜åŒ–
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.set_float32_matmul_precision('medium')
        
        # ðŸ”¥ æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
    
    # ðŸ”¥ å‡å°‘workeræ•°é‡ï¼Œé¿å…å†…å­˜ç“¶é¢ˆ
    if cfg.train.num_workers > 4:
        cfg.train.num_workers = 4
        print(f"âš ï¸ é™åˆ¶workeræ•°é‡ä¸º4ä»¥æå‡æ€§èƒ½")
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(cfg.seed)
    
    # ðŸ”¥ ç®€åŒ–äº¤å‰éªŒè¯ - å‡å°‘foldæ•°é‡ç”¨äºŽå¿«é€Ÿæµ‹è¯•
    n_folds = min(cfg.train.valid_method, 10)  # ðŸ”¥ æœ€å¤š5æŠ˜
    print(f"ðŸ“Š ç®€åŒ–äº¤å‰éªŒè¯: {n_folds}æŠ˜ (åŽŸ{cfg.train.valid_method}æŠ˜)")
    
    n_per = cfg.data.n_subs // n_folds
    remainder = cfg.data.n_subs % n_folds
    
    print(f"ðŸ“Š {n_folds}æŠ˜äº¤å‰éªŒè¯é…ç½®:")
    print(f"   æ•°æ®é›†: {cfg.data.dataset_name}")
    print(f"   æ€»è¢«è¯•æ•°: {cfg.data.n_subs}")
    print(f"   äº¤å‰éªŒè¯: {n_folds}æŠ˜")
    print(f"   æ¯æŠ˜è¢«è¯•æ•°: {n_per}")
    if remainder > 0:
        print(f"   æœ€åŽä¸€æŠ˜é¢å¤–è¢«è¯•æ•°: {remainder}")
    print(f"   è¾“å…¥ç»´åº¦: ({cfg.model.n_channels}, {cfg.model.n_timepoints})")
    print(f"   æ‰¹æ¬¡å¤§å°: {cfg.data.batch_size}")
    print(f"   æœ€å¤§è½®æ¬¡: {cfg.train.max_epochs}")
    print(f"   å­¦ä¹ çŽ‡: {cfg.train.lr}")
    
    fold_results = {
        'train_acc': [],
        'val_acc': [],
        'train_loss': [],
        'val_loss': []
    }
    
    # ðŸ”¥ äº¤å‰éªŒè¯ä¸»å¾ªçŽ¯
    for fold in range(n_folds):
        print(f"\nðŸ”„ å¼€å§‹ç¬¬ {fold+1}/{n_folds} æŠ˜è®­ç»ƒ...")
        
        # ðŸ”¥ æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æ•°æ®åˆ’åˆ†
        start_idx = fold * n_per
        if fold == n_folds - 1:
            end_idx = cfg.data.n_subs
        else:
            end_idx = (fold + 1) * n_per
            
        val_subs = np.arange(start_idx, end_idx)
        train_subs = np.concatenate([
            np.arange(0, start_idx),
            np.arange(end_idx, cfg.data.n_subs)
        ])
        
        print(f"   éªŒè¯è¢«è¯•: {len(val_subs)} äºº (ç´¢å¼•: {start_idx}-{end_idx-1})")
        print(f"   è®­ç»ƒè¢«è¯•: {len(train_subs)} äºº")
        print(f"   éªŒè¯è¢«è¯•ID: {val_subs.tolist()}")
        
        # è®¾ç½®æ£€æŸ¥ç‚¹ç›®å½•
        cp_dir = os.path.join(cfg.log.cp_dir, 'binary_' + cfg.data.dataset_name +'_' + cfg.model.model_type, f'r{cfg.log.run}')
        os.makedirs(cp_dir, exist_ok=True)
        
        # ðŸ”¥ ç®€åŒ–wandbè®¾ç½®
        wandb_logger = WandbLogger(
            name=f"binary_fold{fold+1}_r{cfg.log.run}",
            project=cfg.log.proj_name,
            log_model=False,  # ðŸ”¥ ä¸ä¿å­˜æ¨¡åž‹åˆ°wandbï¼ŒèŠ‚çœæ—¶é—´
            offline=True  # ðŸ”¥ ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œå»¶è¿Ÿ
        )

        # ðŸ”¥ å›žè°ƒä¼˜åŒ–
        checkpoint_callback = ModelCheckpoint(
            monitor="ext/val/acc", 
            mode="max", 
            verbose=False,  # ðŸ”¥ å‡å°‘æ—¥å¿—è¾“å‡º
            dirpath=cp_dir, 
            filename=f'fold{fold+1}_best',
            save_top_k=1,
            save_last=False  # ðŸ”¥ ä¸ä¿å­˜æœ€åŽä¸€ä¸ªæ¨¡åž‹
        )
        
        earlyStopping_callback = EarlyStopping(
            monitor="ext/val/acc", 
            mode="max", 
            patience=min(cfg.train.patience, 20),  # ðŸ”¥ å‡å°‘patience
            verbose=False
        )

        # è§†é¢‘è®¾ç½®
        n_vids = 24 if cfg.data.dataset_name == 'FACED' else cfg.data.n_vids
        train_vids = np.arange(n_vids)
        val_vids = np.arange(n_vids)

        # ðŸ”¥ åˆ›å»ºæ•°æ®æ¨¡å— - å‡å°‘worker
        dm = EEGBinaryDataModule(
            cfg.data, train_subs.tolist(), val_subs.tolist(), 
            train_vids, val_vids, False, 
            min(cfg.train.num_workers, 2)  # ðŸ”¥ æœ€å¤š2ä¸ªworker
        )

        # åˆ›å»ºæ¨¡åž‹
        print(f"ðŸ”§ Fold {fold+1} æ¨¡åž‹ä¿¡æ¯:")
        model = create_binary_model(cfg)
        
        # ðŸ”¥ è®¡ç®—æ¨¡åž‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        model_size_mb = total_params * 4 / (1024 * 1024)  # å‡è®¾float32
        print(f"   å‚æ•°é‡: {total_params:,}")
        print(f"   æ¨¡åž‹å¤§å°: {model_size_mb:.2f} MB")
        
        Extractor = BinaryExtractorModel(model, cfg.train)
        
        # ðŸ”¥ è®­ç»ƒå™¨ä¼˜åŒ–
        trainer = pl.Trainer(
            logger=wandb_logger, 
            callbacks=[checkpoint_callback, earlyStopping_callback],
            max_epochs=min(cfg.train.max_epochs, 100),  # ðŸ”¥ é™åˆ¶æœ€å¤§è½®æ¬¡
            min_epochs=cfg.train.min_epochs, 
            accelerator='gpu' if torch.cuda.is_available() else 'cpu', 
            devices=1,  # ðŸ”¥ åªç”¨ä¸€ä¸ªGPU
            precision='32-true',  # 
            limit_val_batches=1.0,  # ðŸ”¥ ä½¿ç”¨æ‰€æœ‰éªŒè¯æ‰¹æ¬¡
            val_check_interval=1.0,  # ðŸ”¥ æ¯åŠä¸ªepochéªŒè¯ä¸€æ¬¡
            enable_progress_bar=True,
            enable_model_summary=False,  # ðŸ”¥ ä¸æ˜¾ç¤ºæ¨¡åž‹æ‘˜è¦
            deterministic=False,
            gradient_clip_val=1.0,  # ðŸ”¥ æ¢¯åº¦è£å‰ª
            log_every_n_steps=100,  # ðŸ”¥ å‡å°‘æ—¥å¿—é¢‘çŽ‡
            sync_batchnorm=False,  # ðŸ”¥ ä¸åŒæ­¥BN
            fast_dev_run=False
        )
        
        print(f"ðŸŽ¯ å¼€å§‹Fold {fold+1}è®­ç»ƒ...")
        try:
            trainer.fit(Extractor, dm)
            
            # ðŸ”¥ ä¼˜å…ˆä½¿ç”¨æ¨¡åž‹è‡ªå·±è®°å½•çš„æœ€ä½³æŒ‡æ ‡
            if hasattr(Extractor, 'get_best_metrics'):
                best_metrics = Extractor.get_best_metrics()
                if best_metrics:
                    print(f"\nðŸ“Š ä½¿ç”¨æ¨¡åž‹è®°å½•çš„æœ€ä½³æŒ‡æ ‡:")
                    print(f"   æœ€ä½³epoch: {best_metrics.get('epoch', 'unknown')}")
                    print(f"   éªŒè¯å‡†ç¡®çŽ‡: {best_metrics.get('val_acc', 0.0):.4f}")
                    print(f"   éªŒè¯æŸå¤±: {best_metrics.get('val_loss', 0.0):.4f}")
                    print(f"   æœ€ä¼˜é˜ˆå€¼: {best_metrics.get('optimal_threshold', 0.5):.4f}")
                    
                    best_val_acc = best_metrics.get('val_acc', 0.0)
                    best_val_loss = best_metrics.get('val_loss', 0.0)
                    
                    # ðŸ”¥ æ”¶é›†ç»“æžœ
                    if best_val_acc > 0:
                        fold_results['val_acc'].append(best_val_acc)
                    if best_val_loss < float('inf'):
                        fold_results['val_loss'].append(best_val_loss)
                    
                    print(f"âœ… Fold {fold+1} çœŸå®žæœ€ä½³éªŒè¯å‡†ç¡®çŽ‡: {best_val_acc:.4f}")
                    
                    # ðŸ”¥ ä½¿ç”¨å½“å‰è®­ç»ƒå¥½çš„æ¨¡åž‹è¿›è¡Œåˆ†æžï¼ˆå®ƒå·²ç»æ˜¯ç»è¿‡å®Œæ•´è®­ç»ƒçš„ï¼‰
                    print(f"\nðŸ” Fold {fold+1} é¢„æµ‹åˆ†æž (å®Œæ•´è®­ç»ƒæ¨¡åž‹):")
                    Extractor.eval()
                    val_loader = dm.val_dataloader()
                    val_predictions, val_targets, val_probs, val_logits = analyze_predictions(
                        Extractor, val_loader, "éªŒè¯é›†"
                    )
                    
                    # ç»§ç»­ä¸‹ä¸€ä¸ªfold
                    continue
            
            # ðŸ”¥ å¦‚æžœæ¨¡åž‹æ²¡æœ‰è®°å½•ï¼Œä½¿ç”¨åŽŸæ¥çš„é€»è¾‘
            # ... existing checkpoint logic ...
            # ðŸ”¥ èŽ·å–æœ€ä½³ç»“æžœ
            best_val_acc = 0.0
            best_train_acc = 0.0
            best_val_loss = float('inf')
            best_train_loss = float('inf')
            
            # ðŸ”¥ æ–¹æ³•1ï¼šä»ŽModelCheckpointèŽ·å–æœ€ä½³éªŒè¯å‡†ç¡®çŽ‡
            if checkpoint_callback.best_model_path:
                best_val_acc = float(checkpoint_callback.best_model_score)
                print(f"ðŸ“Š æœ€ä½³éªŒè¯å‡†ç¡®çŽ‡: {best_val_acc:.4f}")
                
                # ðŸ”¥ åŠ è½½æœ€ä½³æ¨¡åž‹æ¥èŽ·å–å®Œæ•´æŒ‡æ ‡
                try:
                    best_model = BinaryExtractorModel.load_from_checkpoint(
                        checkpoint_callback.best_model_path,
                        model=model,
                        cfg_train=cfg.train
                    )
                    
                    # ðŸ”¥ é‡æ–°åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
                    best_model.eval()
                    val_results = trainer.validate(best_model, dm, verbose=False)
                    
                    if val_results:
                        best_val_loss = float(trainer.callback_metrics.get('ext/val/loss', best_val_loss))
                    
                    # ðŸ”¥ åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°ï¼ˆèŽ·å–å¯¹åº”çš„è®­ç»ƒæŒ‡æ ‡ï¼‰
                    train_results = trainer.validate(best_model, dm.train_dataloader(), verbose=False)
                    if train_results:
                        # æ³¨æ„ï¼šè¿™é‡Œå®žé™…ä¸Šæ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸ŠåšéªŒè¯ï¼Œæ‰€ä»¥æŒ‡æ ‡åå¯èƒ½ä¸åŒ
                        train_metrics = trainer.callback_metrics
                        for key, value in train_metrics.items():
                            if 'acc' in key:
                                best_train_acc = float(value)
                            elif 'loss' in key:
                                best_train_loss = float(value)
                        
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½æœ€ä½³æ¨¡åž‹å¤±è´¥: {e}")
                    # ä½¿ç”¨å½“å‰æ¨¡åž‹çš„ç»“æžœ
                    best_val_acc = float(trainer.callback_metrics.get('ext/val/acc', 0.0))
                    best_val_loss = float(trainer.callback_metrics.get('ext/val/loss', 0.0))
                    best_train_acc = float(trainer.callback_metrics.get('ext/train/acc', 0.0))
                    best_train_loss = float(trainer.callback_metrics.get('ext/train/loss', 0.0))
            
            else:
                # ðŸ”¥ å¦‚æžœæ²¡æœ‰æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨æœ€åŽçš„ç»“æžœ
                metrics = trainer.callback_metrics
                best_val_acc = float(metrics.get('ext/val/acc', 0.0))
                best_val_loss = float(metrics.get('ext/val/loss', 0.0))
                best_train_acc = float(metrics.get('ext/train/acc', 0.0))
                best_train_loss = float(metrics.get('ext/train/loss', 0.0))
            
            # ðŸ”¥ æ˜¾ç¤ºæœ€ä½³ç»“æžœ
            print(f"\nðŸ“Š Fold {fold+1} æœ€ä½³ç»“æžœ:")
            print(f"   ðŸ“ˆ è®­ç»ƒ: å‡†ç¡®çŽ‡={best_train_acc:.4f}, æŸå¤±={best_train_loss:.4f}")
            print(f"   ðŸ“Š éªŒè¯: å‡†ç¡®çŽ‡={best_val_acc:.4f}, æŸå¤±={best_val_loss:.4f}")
            
            # ðŸ”¥ æ”¶é›†æœ€ä½³ç»“æžœ
            if best_val_acc > 0:
                fold_results['val_acc'].append(best_val_acc)
            if best_val_loss < float('inf'):
                fold_results['val_loss'].append(best_val_loss)
            if best_train_acc > 0:
                fold_results['train_acc'].append(best_train_acc)
            if best_train_loss < float('inf'):
                fold_results['train_loss'].append(best_train_loss)
            
            print(f"âœ… Fold {fold+1} æœ€ä½³éªŒè¯å‡†ç¡®çŽ‡: {best_val_acc:.4f}")
            
            # ðŸ”¥ ä½¿ç”¨æœ€ä½³æ¨¡åž‹è¿›è¡Œé¢„æµ‹åˆ†æž
            if checkpoint_callback.best_model_path and os.path.exists(checkpoint_callback.best_model_path):
                try:
                    print(f"\nðŸ” Fold {fold+1} é¢„æµ‹åˆ†æž (æœ€ä½³æ¨¡åž‹):")
                    best_model = BinaryExtractorModel.load_from_checkpoint(
                        checkpoint_callback.best_model_path,
                        model=model,
                        cfg_train=cfg.train
                    )
                    best_model.eval()
                    
                    # åˆ†æžéªŒè¯é›†é¢„æµ‹
                    val_loader = dm.val_dataloader()
                    val_predictions, val_targets, val_probs, val_logits = analyze_predictions(
                        best_model, val_loader, "éªŒè¯é›†"
                    )
                    
                except Exception as e:
                    print(f"âš ï¸ æœ€ä½³æ¨¡åž‹åˆ†æžå¤±è´¥: {e}")
            else:
                # ä½¿ç”¨å½“å‰æ¨¡åž‹
                print(f"\nðŸ” Fold {fold+1} é¢„æµ‹åˆ†æž (å½“å‰æ¨¡åž‹):")
                Extractor.eval()
                val_loader = dm.val_dataloader()
                val_predictions, val_targets, val_probs, val_logits = analyze_predictions(
                    Extractor, val_loader, "éªŒè¯é›†"
                )
            
        except Exception as e:
            print(f"âŒ Fold {fold+1} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

        # ðŸ”¥ æ¸…ç†wandb
        try:
            wandb.finish()
        except:
            pass
        
        # ðŸ”¥ æµ‹è¯•æ¨¡å¼åªè·‘ä¸€æŠ˜
        if cfg.train.iftest:
            print("ðŸ§ª æµ‹è¯•æ¨¡å¼ï¼Œåªè¿è¡Œä¸€æŠ˜")
            break
    
    # è®¡ç®—æ€»ä½“ç»“æžœ
    if len(fold_results['val_acc']) > 0:
        mean_val_acc = np.mean(fold_results['val_acc'])
        std_val_acc = np.std(fold_results['val_acc'])
        
        print(f"\nðŸŽ‰ äº¤å‰éªŒè¯ç»“æžœ:")
        print(f"   éªŒè¯å‡†ç¡®çŽ‡: {mean_val_acc:.4f} Â± {std_val_acc:.4f}")
        print(f"   æˆåŠŸfoldæ•°: {len(fold_results['val_acc'])}/{n_folds}")
    else:
        print(f"\nâŒ æ²¡æœ‰æˆåŠŸçš„fold")

def analyze_predictions(model, dataloader, dataset_name):
    """åˆ†æžæ¨¡åž‹é¢„æµ‹ç»“æžœ"""
    import torch
    import numpy as np
    from sklearn.metrics import confusion_matrix, classification_report
    
    model.eval()
    all_predictions = []
    all_targets = []
    all_probs = []
    all_logits = []
    
    with torch.no_grad():
        for batch_idx, (x, y) in enumerate(dataloader):
            if batch_idx >= 10:  # åªåˆ†æžå‰10ä¸ªbatchï¼Œé¿å…å¤ªæ…¢
                break
                
            x = x.to(model.device)
            y = y.to(model.device)
            
            # èŽ·å–æ¨¡åž‹è¾“å‡º
            logits = model.forward(x).squeeze(1)
            probs = torch.sigmoid(logits)
            predictions = (probs > 0.5).float()
            
            all_logits.extend(logits.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(y.cpu().numpy())
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_logits = np.array(all_logits)
    all_probs = np.array(all_probs)
    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)
    
    print(f"   ðŸ“Š {dataset_name} é¢„æµ‹åˆ†å¸ƒ:")
    print(f"      æ ·æœ¬æ•°: {len(all_targets)}")
    print(f"      çœŸå®žæ ‡ç­¾åˆ†å¸ƒ: è´Ÿæ ·æœ¬={np.sum(all_targets==0)}, æ­£æ ·æœ¬={np.sum(all_targets==1)}")
    print(f"      é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ: è´Ÿæ ·æœ¬={np.sum(all_predictions==0)}, æ­£æ ·æœ¬={np.sum(all_predictions==1)}")
    
    # print(f"   ðŸ“ˆ Logitsç»Ÿè®¡:")
    # print(f"      èŒƒå›´: [{all_logits.min():.3f}, {all_logits.max():.3f}]")
    # print(f"      å‡å€¼: {all_logits.mean():.3f} Â± {all_logits.std():.3f}")
    
    print(f"   ðŸ“ˆ æ¦‚çŽ‡ç»Ÿè®¡:èŒƒå›´: [{all_probs.min():.3f}, {all_probs.max():.3f}],å‡å€¼: {all_probs.mean():.3f} Â± {all_probs.std():.3f}")
    # print(f"      èŒƒå›´: [{all_probs.min():.3f}, {all_probs.max():.3f}]")
    # print(f"      å‡å€¼: {all_probs.mean():.3f} Â± {all_probs.std():.3f}")
    
    # æŒ‰ç±»åˆ«åˆ†æž
    if len(np.unique(all_targets)) > 1:
        pos_indices = all_targets == 1
        neg_indices = all_targets == 0
        
        if np.any(pos_indices):
            pos_probs = all_probs[pos_indices]
            pos_logits = all_logits[pos_indices]
            print(f"   ðŸ“Š æ­£æ ·æœ¬ç»Ÿè®¡:")
            print(f"      æ¦‚çŽ‡: {pos_probs.mean():.3f} Â± {pos_probs.std():.3f}")
            print(f"      logits: {pos_logits.mean():.3f} Â± {pos_logits.std():.3f}")
        
        if np.any(neg_indices):
            neg_probs = all_probs[neg_indices]
            neg_logits = all_logits[neg_indices]
            # print(f"   ðŸ“Š è´Ÿæ ·æœ¬ç»Ÿè®¡:")
            # print(f"      æ¦‚çŽ‡: {neg_probs.mean():.3f} Â± {neg_probs.std():.3f}")
            # print(f"      logits: {neg_logits.mean():.3f} Â± {neg_logits.std():.3f}")
        
        # è®¡ç®—åˆ†ç¦»åº¦
        if np.any(pos_indices) and np.any(neg_indices):
            prob_separation = abs(pos_probs.mean() - neg_probs.mean())
            logit_separation = abs(pos_logits.mean() - neg_logits.mean())
            print(f"   ðŸŽ¯ ç±»åˆ«åˆ†ç¦»åº¦:")
            print(f"      æ¦‚çŽ‡åˆ†ç¦»: {prob_separation:.3f}")
            print(f"      logitsåˆ†ç¦»: {logit_separation:.3f}")
            
            if prob_separation < 0.1:
                print(f"      âš ï¸ æ¦‚çŽ‡åˆ†ç¦»åº¦è¿‡å°ï¼Œæ¨¡åž‹å¯èƒ½æ²¡æœ‰å­¦ä¼šåŒºåˆ†")
            else:
                print(f"      âœ… æ¦‚çŽ‡åˆ†ç¦»åº¦è‰¯å¥½")
    
    # æ··æ·†çŸ©é˜µ
    if len(np.unique(all_targets)) > 1 and len(np.unique(all_predictions)) > 1:
        cm = confusion_matrix(all_targets, all_predictions)
        print(f"   ðŸ“Š æ··æ·†çŸ©é˜µ:")
        print(f"      TN={cm[0,0]}, FP={cm[0,1]}")
        print(f"      FN={cm[1,0]}, TP={cm[1,1]}")
    
    return all_predictions, all_targets, all_probs, all_logits

if __name__ == "__main__":
    train_binary_ext()