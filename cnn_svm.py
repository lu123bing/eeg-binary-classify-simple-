# cnn_svm/eeg_binary_classifier.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, classification_report
import logging
import hydra
from omegaconf import DictConfig
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
import wandb
from data_loader import create_data_loaders

# ğŸ”¥ ç®€å•æ£€æŸ¥
print(f"ğŸ” CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

log = logging.getLogger(__name__)

class EEGBinaryDataset(Dataset):
    """ç®€å•çš„EEGæ•°æ®é›†"""
    
    def __init__(self, data, labels):
        self.data = torch.FloatTensor(data)
        self.labels = torch.LongTensor(labels)
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = self.data[idx]  # (17, 250)
        y = self.labels[idx]
        return x, y

class SimpleEEGNet(nn.Module):
    """ç®€æ´çš„EEGNet - ä¸“é—¨ä¸ºEEGè®¾è®¡"""
    
    def __init__(self, n_channels=17, n_timepoints=250, n_classes=2, dropout=0.25):
        super().__init__()
        
        # ğŸ”¥ EEGNet Block 1 - æ—¶é—´å·ç§¯
        self.block1 = nn.Sequential(
            nn.Conv2d(1, 16, (1, 64), padding=(0, 32), bias=False),
            nn.BatchNorm2d(16),
        )
        
        # ğŸ”¥ æ·±åº¦å·ç§¯ - æ¯ä¸ªé€šé“å•ç‹¬å·ç§¯
        self.depthwise = nn.Sequential(
            nn.Conv2d(16, 32, (n_channels, 1), groups=16, bias=False),
            nn.BatchNorm2d(32),
            nn.ELU(),
            nn.AvgPool2d((1, 4)),
            nn.Dropout(dropout)
        )
        
        # ğŸ”¥ EEGNet Block 2 - å¯åˆ†ç¦»å·ç§¯
        self.block2 = nn.Sequential(
            nn.Conv2d(32, 32, (1, 16), padding=(0, 8), groups=32, bias=False),
            nn.Conv2d(32, 16, (1, 1), bias=False),
            nn.BatchNorm2d(16),
            nn.ELU(),
            nn.AvgPool2d((1, 8)),
            nn.Dropout(dropout)
        )
        
        # ğŸ”¥ è®¡ç®—ç‰¹å¾ç»´åº¦
        self.feature_dim = self._get_feature_dim(n_channels, n_timepoints)
        
        # ğŸ”¥ åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.feature_dim, n_classes)
        )
        
    def _get_feature_dim(self, n_channels, n_timepoints):
        """è®¡ç®—è¾“å‡ºç‰¹å¾ç»´åº¦"""
        x = torch.randn(1, 1, n_channels, n_timepoints)
        with torch.no_grad():
            x = self.block1(x)
            x = self.depthwise(x)
            x = self.block2(x)
        return x.numel()
    
    def forward(self, x):
        # è¾“å…¥: (batch, 17, 250)
        if x.dim() == 3:
            x = x.unsqueeze(1)  # (batch, 1, 17, 250)
        
        x = self.block1(x)
        x = self.depthwise(x)
        x = self.block2(x)
        x = self.classifier(x)
        
        return x

class EEGBinaryModel(pl.LightningModule):
    """ç®€å•çš„EEGäºŒåˆ†ç±»æ¨¡å‹"""
    
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.save_hyperparameters()
        
        # ğŸ”¥ ä½¿ç”¨ç®€å•çš„EEGNet
        self.model = SimpleEEGNet(
            n_channels=cfg.model.n_channels,
            n_timepoints=cfg.model.n_timepoints,
            n_classes=cfg.model.n_classes,
            dropout=cfg.model.dropout
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
        
    def forward(self, x):
        return self.model(x)
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # è®¡ç®—å‡†ç¡®ç‡
        preds = torch.argmax(logits, dim=1)
        acc = (preds == y).float().mean()
        
        self.log_dict({
            'train/loss': loss,
            'train/acc': acc,
        }, on_step=False, on_epoch=True, prog_bar=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # è®¡ç®—å‡†ç¡®ç‡
        preds = torch.argmax(logits, dim=1)
        acc = (preds == y).float().mean()
        
        self.log_dict({
            'val/loss': loss,
            'val/acc': acc,
        }, on_epoch=True, prog_bar=True)
        
        return loss
    
    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        preds = torch.argmax(logits, dim=1)
        return {'preds': preds, 'targets': y}
    
    def configure_optimizers(self):
        # ğŸ”¥ ç®€å•çš„Adamä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            self.parameters(), 
            lr=self.cfg.train.lr,
            weight_decay=self.cfg.train.weight_decay
        )
        
        # ğŸ”¥ ç®€å•çš„å­¦ä¹ ç‡è°ƒåº¦
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer, step_size=30, gamma=0.5
        )
        
        return [optimizer], [scheduler]

@hydra.main(config_path="../cfgs", config_name="binary_config", version_base="1.3")
def train_binary_classifier(cfg: DictConfig):
    """è®­ç»ƒç®€å•çš„EEGäºŒåˆ†ç±»å™¨"""
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒç®€å•EEGåˆ†ç±»å™¨")
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(cfg.seed)
    
    # ğŸ”¥ ç®€å•çš„æ•°æ®åŠ è½½
    data_dir = cfg.data.data_dir
    train_loader, val_loader, test_loader = create_data_loaders(
        data_dir=data_dir,
        batch_size=cfg.train.batch_size,
        num_workers=cfg.train.num_workers,
        max_samples=cfg.data.get('max_samples', None)
    )
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"   éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
    print(f"   æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = EEGBinaryModel(cfg)
    
    # ğŸ”¥ ç®€å•çš„å›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        monitor='val/acc',
        mode='max',
        save_top_k=1,
        filename='best-{epoch:02d}-{val_acc:.3f}'
    )
    
    early_stopping = EarlyStopping(
        monitor='val/acc',
        mode='max',
        patience=cfg.train.patience,
        verbose=True
    )
    
    # ğŸ”¥ ç®€å•çš„æ—¥å¿—
    wandb_logger = WandbLogger(
        name=f"simple_eegnet_{cfg.model.n_channels}ch",
        project="simple_eeg_classification"
    )
    
    # ğŸ”¥ ç®€å•çš„è®­ç»ƒå™¨
    trainer = pl.Trainer(
        logger=wandb_logger,
        callbacks=[checkpoint_callback, early_stopping],
        max_epochs=cfg.train.max_epochs,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        devices=1,
        precision='16-mixed' if torch.cuda.is_available() else 32,
        log_every_n_steps=50,
        check_val_every_n_epoch=1,
    )
    
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    trainer.fit(model, train_loader, val_loader)
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
    test_results = trainer.test(model, test_loader, ckpt_path='best')
    
    # ğŸ”¥ ç®€å•çš„æµ‹è¯•è¯„ä¼°
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for batch in test_loader:
            x, y = batch
            if torch.cuda.is_available():
                x = x.cuda()
                
            logits = model(x)
            preds = torch.argmax(logits, dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(y.numpy())
    
    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
    final_acc = accuracy_score(all_targets, all_preds)
    print(f"\nğŸ‰ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.4f} ({final_acc*100:.2f}%)")
    
    # ğŸ”¥ ç®€å•çš„åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(
        all_targets, all_preds, 
        target_names=['è´Ÿé¢', 'æ­£é¢'],
        digits=4
    ))
    
    wandb.finish()
    return model, final_acc

if __name__ == "__main__":
    train_binary_classifier()