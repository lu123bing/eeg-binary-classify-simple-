# cnn_svm/data_loader.py
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import pickle
from tqdm import tqdm

def create_binary_datasets(data_dir=None, train_subs=None, val_subs=None, max_samples=None, 
                          n_channels=17, n_timepoints=250):
    """åˆ›å»ºäºŒåˆ†ç±»æ•°æ®é›† - ç®€å•å‚æ•°ç‰ˆæœ¬"""
    
    print("ğŸ”„ åˆ›å»ºäºŒåˆ†ç±»æ•°æ®é›† (ä½¿ç”¨é¢„å¤„ç†æ•°æ®)...")
    print(f"   æ•°æ®ç›®å½•: {data_dir}")
    print(f"   é…ç½®: {n_channels}é€šé“, {n_timepoints}æ—¶é—´ç‚¹")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return create_dummy_datasets(n_channels, n_timepoints)
    
    # åŠ è½½æ•°æ®
    train_dataset, val_dataset, test_dataset = load_preprocessed_eeg_data(
        data_dir, train_subs, val_subs, max_samples, n_channels, n_timepoints
    )
    
    return train_dataset, val_dataset, test_dataset

def load_preprocessed_eeg_data(data_dir, train_subs=None, val_subs=None, max_samples=None,
                              n_channels=17, n_timepoints=500):
    """åŠ è½½é¢„å¤„ç†åçš„EEGæ•°æ®"""
    
    # åŠ è½½å…ƒæ•°æ®
    metadata_file = os.path.join(data_dir, 'metadata.pkl')
    if not os.path.exists(metadata_file):
        print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
        return create_dummy_datasets(n_channels, n_timepoints)
    
    print(f"ğŸ“‚ åŠ è½½å…ƒæ•°æ®: {metadata_file}")
    with open(metadata_file, 'rb') as f:
        metadata = pickle.load(f)
    
    # print(f"ğŸ“Š å…ƒæ•°æ®ä¿¡æ¯:")
    # print(f"   è¢«è¯•æ•°: {len(metadata['subject_info'])}")
    # print(f"   è§†é¢‘æ ·æœ¬æ•°: {len(metadata['video_info'])}")
    # print(f"   æ ‡ç­¾æ˜ å°„: {metadata['label_mapping']}")
    
    # æ ¹æ®è¢«è¯•åˆ’åˆ†è¿‡æ»¤æ•°æ®
    if train_subs is not None or val_subs is not None:
        train_files, val_files = filter_by_subjects(metadata['video_info'], train_subs, val_subs)
    else:
        # æ²¡æœ‰æŒ‡å®šè¢«è¯•åˆ’åˆ†ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®
        all_files = metadata['video_info']
        train_files = val_files = all_files
    
    # print(f"ğŸ“Š æ•°æ®æ–‡ä»¶ç»Ÿè®¡:")
    # print(f"   è®­ç»ƒæ–‡ä»¶: {len(train_files) if train_files else 0}")
    # print(f"   éªŒè¯æ–‡ä»¶: {len(val_files) if val_files else 0}")
    
    # åŠ è½½æ•°æ®æ–‡ä»¶
    if train_files:
        train_data, train_labels = load_data_files(
            data_dir, train_files, max_samples, "è®­ç»ƒ", n_channels, n_timepoints
        )
    else:
        train_data, train_labels = [], []
    
    # ğŸ”¥ åŠ è½½éªŒè¯æ•°æ®
    if val_files and train_subs != val_subs:  # é¿å…é‡å¤åŠ è½½ç›¸åŒæ•°æ®
        val_data, val_labels = load_data_files(
            data_dir, val_files, max_samples, "éªŒè¯", n_channels, n_timepoints
        )
    else:
        val_data, val_labels = train_data, train_labels
    
    # ğŸ”¥ æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if len(train_data) == 0:
        print("âš ï¸ æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return create_dummy_datasets(n_channels, n_timepoints)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    train_data = np.array(train_data, dtype=np.float32)
    train_labels = np.array(train_labels, dtype=np.int64)
    val_data = np.array(val_data, dtype=np.float32)
    val_labels = np.array(val_labels, dtype=np.int64)
    
    print(f"ğŸ“Š åŠ è½½åæ•°æ®ç»Ÿè®¡:")
    print(f"   è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data.shape},æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(train_labels)}")
    print(f"   è®­ç»ƒæ ‡ç­¾åˆ†å¸ƒ: {np.bincount(train_labels)}")
    # print(f"   éªŒè¯æ•°æ®å½¢çŠ¶: {val_data.shape}")
    # print(f"   éªŒè¯æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(val_labels)}")
    
    # å¦‚æœéªŒè¯æ•°æ®å’Œè®­ç»ƒæ•°æ®ç›¸åŒï¼Œè¿›è¡Œåˆ’åˆ†
    if np.array_equal(train_data, val_data):
        train_data, val_data, train_labels, val_labels = train_test_split(
            train_data, train_labels, 
            test_size=0.2, 
            random_state=42, 
            stratify=train_labels
        )
        print(f"ğŸ“Š é‡æ–°åˆ’åˆ†å:")
        print(f"   è®­ç»ƒæ•°æ®: {train_data.shape}")
        print(f"   éªŒè¯æ•°æ®: {val_data.shape}")
    
    # åˆ›å»ºæµ‹è¯•é›†ï¼ˆä»éªŒè¯é›†åˆ†ç¦»ï¼‰
    if len(val_data) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®åˆ†ç¦»æµ‹è¯•é›†
        val_data, test_data, val_labels, test_labels = train_test_split(
            val_data, val_labels,
            test_size=0.5,
            random_state=42,
            stratify=val_labels
        )
    else:
        test_data, test_labels = val_data.copy(), val_labels.copy()
    
    # print(f"ğŸ“Š æœ€ç»ˆæ•°æ®é›†åˆ’åˆ†:")
    # print(f"   è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    # print(f"   éªŒè¯é›†: {len(val_data)} æ ·æœ¬") 
    # print(f"   æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = EEGBinaryDataset(train_data, train_labels, "è®­ç»ƒ")
    val_dataset = EEGBinaryDataset(val_data, val_labels, "éªŒè¯")
    test_dataset = EEGBinaryDataset(test_data, test_labels, "æµ‹è¯•")
    
    return train_dataset, val_dataset, test_dataset

def filter_by_subjects(video_info_list, train_subs, val_subs):
    """æ ¹æ®è¢«è¯•IDè¿‡æ»¤æ•°æ®æ–‡ä»¶"""
    
    train_files = []
    val_files = []
    
    # ğŸ”¥ ç¡®ä¿è¢«è¯•åˆ—è¡¨æ˜¯åˆ—è¡¨æ ¼å¼
    if train_subs is not None:
        if isinstance(train_subs, np.ndarray):
            train_subs = train_subs.tolist()
        elif not isinstance(train_subs, list):
            train_subs = [train_subs]
    
    if val_subs is not None:
        if isinstance(val_subs, np.ndarray):
            val_subs = val_subs.tolist()
        elif not isinstance(val_subs, list):
            val_subs = [val_subs]
    
    # ğŸ”¥ éå†æ‰€æœ‰è§†é¢‘ä¿¡æ¯ï¼Œæ ¹æ®è¢«è¯•IDåˆ†ç±»
    for info in video_info_list:
        subject_id = info['subject_id']
        
        if train_subs is not None and subject_id in train_subs:
            train_files.append(info)
        elif val_subs is not None and subject_id in val_subs:
            val_files.append(info)
        elif train_subs is None and val_subs is None:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ’åˆ†ï¼Œæ‰€æœ‰æ•°æ®éƒ½ç”¨äºè®­ç»ƒå’ŒéªŒè¯
            train_files.append(info)
            val_files.append(info)
    
    print(f"ğŸ“Š è¢«è¯•ç­›é€‰ç»“æœ:")
    if train_subs:
        train_subjects = set(info['subject_id'] for info in train_files)
        # print(f"   è®­ç»ƒè¢«è¯•: {sorted(train_subjects)} ({len(train_subjects)}ä¸ª)")
    if val_subs:
        val_subjects = set(info['subject_id'] for info in val_files)
        print(f"   éªŒè¯è¢«è¯•: {sorted(val_subjects)} ({len(val_subjects)}ä¸ª)")
    
    return train_files, val_files

def load_data_files(data_dir, file_info_list, max_samples=None, dataset_name="",
                   n_channels=17, n_timepoints=500):
    """åŠ è½½æŒ‡å®šçš„æ•°æ®æ–‡ä»¶"""
    
    if max_samples and len(file_info_list) > max_samples:
        # éšæœºé€‰æ‹©æ–‡ä»¶
        np.random.seed(42)
        selected_indices = np.random.choice(len(file_info_list), max_samples, replace=False)
        file_info_list = [file_info_list[i] for i in selected_indices]
        print(f"âš ï¸ {dataset_name}æ•°æ®é™åˆ¶åŠ è½½: {max_samples} ä¸ªæ–‡ä»¶")
    
    all_data = []
    all_labels = []
    success_count = 0
    failed_count = 0
    
    print(f"ğŸ“¥ åŠ è½½{dataset_name}æ•°æ®: {len(file_info_list)} ä¸ªæ–‡ä»¶")
    
    expected_shape = (n_channels, n_timepoints)
    
    for info in tqdm(file_info_list, desc=f"åŠ è½½{dataset_name}æ•°æ®"):
        try:
            file_path = os.path.join(data_dir, info['filename'])
            
            if not os.path.exists(file_path):
                failed_count += 1
                continue
            
            # ğŸ”¥ åŠ è½½é¢„å¤„ç†åçš„pklæ–‡ä»¶
            with open(file_path, 'rb') as f:
                data_dict = pickle.load(f)
            
            # ğŸ”¥ æå–EEGæ•°æ®å’Œæ ‡ç­¾
            if 'eeg' in data_dict and 'label' in data_dict:
                eeg_signal = data_dict['eeg']
                label = data_dict['label']
                
                # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
                if isinstance(eeg_signal, np.ndarray) and eeg_signal.shape == expected_shape:
                    all_data.append(eeg_signal.astype(np.float32))
                    all_labels.append(int(label))
                    success_count += 1
                else:
                    print(f"âš ï¸ æ•°æ®å½¢çŠ¶ä¸æ­£ç¡®: {info['filename']}, å½¢çŠ¶: {eeg_signal.shape}, æœŸæœ›: {expected_shape}")
                    failed_count += 1
            else:
                print(f"âš ï¸ æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {info['filename']}")
                failed_count += 1
                
        except Exception as e:
            failed_count += 1
            if failed_count <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                print(f"âš ï¸ åŠ è½½å¤±è´¥ {info['filename']}: {e}")
    
    print(f"ğŸ“Š {dataset_name}æ•°æ®åŠ è½½ç»“æœ:")
    print(f"   æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
    print(f"   å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
    
    return all_data, all_labels

def create_dummy_datasets(n_channels=17, n_timepoints=500):
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†ç”¨äºæµ‹è¯•"""
    print("ğŸ¯ åˆ›å»ºæ¨¡æ‹ŸEEGæ•°æ®é›†...")
    print(f"   å‚æ•°: {n_channels}é€šé“, {n_timepoints}æ—¶é—´ç‚¹")
    
    # æ¨¡æ‹Ÿæ•°æ®
    n_samples = 5000
    
    # ç”ŸæˆéšæœºEEGæ•°æ®
    np.random.seed(42)
    dummy_data = np.random.randn(n_samples, n_channels, n_timepoints).astype(np.float32)
    
    # ğŸ”¥ æ·»åŠ ä¸€äº›çœŸå®çš„EEGç‰¹å¾æ¨¡å¼
    for i in range(n_samples):
        # æ·»åŠ Î±æ³¢(8-12Hz)å’ŒÎ²æ³¢(13-30Hz)æ¨¡å¼
        t = np.linspace(0, n_timepoints/250, n_timepoints)  # å‡è®¾250Hzé‡‡æ ·ç‡
        alpha_wave = 0.5 * np.sin(2 * np.pi * 10 * t)  # 10Hz Î±æ³¢
        beta_wave = 0.3 * np.sin(2 * np.pi * 20 * t)   # 20Hz Î²æ³¢
        
        # éšæœºé€‰æ‹©å‡ ä¸ªé€šé“æ·»åŠ æ³¢å½¢
        channels = np.random.choice(n_channels, min(5, n_channels), replace=False)
        for ch in channels:
            dummy_data[i, ch] += alpha_wave + beta_wave
    
    # ç”ŸæˆäºŒåˆ†ç±»æ ‡ç­¾ (0: æ¶ˆæ, 1: ç§¯æ)
    dummy_labels = np.random.randint(0, 2, n_samples).astype(np.int64)
    
    print(f"ğŸ“Š æ¨¡æ‹Ÿæ•°æ®ä¿¡æ¯:")
    print(f"   æ•°æ®å½¢çŠ¶: {dummy_data.shape}")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(dummy_labels)}")
    
    # åˆ’åˆ†æ•°æ®é›†
    train_data, temp_data, train_labels, temp_labels = train_test_split(
        dummy_data, dummy_labels, test_size=0.2, random_state=42, stratify=dummy_labels
    )
    
    val_data, test_data, val_labels, test_labels = train_test_split(
        temp_data, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
    )
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = EEGBinaryDataset(train_data, train_labels, "è®­ç»ƒ(æ¨¡æ‹Ÿ)")
    val_dataset = EEGBinaryDataset(val_data, val_labels, "éªŒè¯(æ¨¡æ‹Ÿ)")
    test_dataset = EEGBinaryDataset(test_data, test_labels, "æµ‹è¯•(æ¨¡æ‹Ÿ)")
    
    return train_dataset, val_dataset, test_dataset

class EEGBinaryDataset(Dataset):
    """EEGäºŒåˆ†ç±»æ•°æ®é›†"""
    
    def __init__(self, data, labels, name=""):
        self.data = torch.FloatTensor(data)  # (N, 17, 500)
        self.labels = torch.LongTensor(labels)  # (N,)
        self.name = name
        
        print(f"ğŸ“¦ {name}æ•°æ®é›†åˆå§‹åŒ–:")
        print(f"   æ•°æ®: {self.data.shape}, dtype: {self.data.dtype}")
        print(f"   æ ‡ç­¾: {self.labels.shape}, dtype: {self.labels.dtype}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(self.labels.numpy())}")
        print(f"   æ•°æ®èŒƒå›´: [{self.data.min():.3f}, {self.data.max():.3f}]")
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]