import torch
import torch.nn as nn
import torch.nn.functional as F

class EEGBinaryModel(nn.Module):
    """EEGäºŒåˆ†ç±»æ¨¡å‹ - ä¿®å¤ä¸ºlogitsè¾“å‡º"""
    
    def __init__(self, n_channels=17, n_timepoints=250, d_model=128, n_classes=2, dropout=0.1):
        super().__init__()
        
        # CNNç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Conv1d(n_channels, 64, kernel_size=7, padding=3),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            
            nn.Conv1d(64, 128, kernel_size=5, padding=2),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),
            
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(16),
            
            nn.Flatten(),
            nn.Linear(256 * 16, d_model),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ğŸ”¥ ä¿®å¤ï¼šè¾“å‡ºlogitsï¼ˆä¸ç”¨sigmoidï¼‰
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 2, 1)  # ğŸ”¥ è¾“å‡ºåŸå§‹logitsï¼Œä¸ç”¨sigmoid
        )
        
        self.stratified = []
        
    def forward(self, x):
        if x.dim() == 4:
            x = x.squeeze(1)
        features = self.encoder(x)
        return features
    
    def predict(self, x):
        """é¢„æµ‹é˜¶æ®µï¼šè¿”å›logits"""
        features = self.forward(x)
        logits = self.classifier(features)
        return logits
    
    def predict_proba(self, x):
        """è¿”å›æ¦‚ç‡"""
        logits = self.predict(x)
        probs = torch.sigmoid(logits)
        return probs
    
    def classify(self, x):
        """åˆ†ç±»é˜¶æ®µï¼šè¿”å›ç±»åˆ«æ ‡ç­¾"""
        probs = self.predict_proba(x)
        return (probs > 0.5).float() 

class EEGNet(nn.Module):
    """EEGNetæ¶æ„ - ä¿®å¤ä¸ºlogitsè¾“å‡º"""
    def __init__(self, n_channels=17, n_timepoints=250, n_classes=2, dropout=0.25):
        super().__init__()
        
        # EEGNetç»“æ„ä¿æŒä¸å˜...
        self.block1 = nn.Sequential(
            nn.Conv2d(1, 16, (1, 64), padding=(0, 32), bias=False),
            nn.BatchNorm2d(16),
            nn.Conv2d(16, 32, (n_channels, 1), groups=16, bias=False),
            nn.BatchNorm2d(32),
            nn.ELU(),
            nn.AvgPool2d((1, 4)),
            nn.Dropout(dropout)
        )
        
        self.block2 = nn.Sequential(
            nn.Conv2d(32, 32, (1, 16), padding=(0, 8), groups=32, bias=False),
            nn.Conv2d(32, 16, 1, bias=False),
            nn.BatchNorm2d(16),
            nn.ELU(),
            nn.AvgPool2d((1, 8)),
            nn.Dropout(dropout)
        )
        
        self.feature_dim = self._get_feature_dim(n_channels, n_timepoints)
        
        # ğŸ”¥ ä¿®å¤ï¼šè¾“å‡ºlogits
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.feature_dim, 64),
            nn.ELU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1)  # ä¸ç”¨sigmoid
        )
        
        # ğŸ”¥ é‡è¦ï¼šåˆå§‹åŒ–æœ€åä¸€å±‚biasä¸ºè´Ÿå€¼
        # è¿™æ ·åˆå§‹é¢„æµ‹ä¼šåå‘è´Ÿç±»ï¼Œé¿å…æ€»æ˜¯é¢„æµ‹æ­£ç±»
        with torch.no_grad():
            self.classifier[-1].bias.fill_(-0.5)  # ğŸ”¥ è´Ÿåç½®
            self.classifier[-1].weight.normal_(0, 0.01)  # ğŸ”¥ å°æƒé‡
        
        self.stratified = []
        
    def _get_feature_dim(self, n_channels, n_timepoints):
        x = torch.randn(1, 1, n_channels, n_timepoints)
        x = self.block1(x)
        x = self.block2(x)
        return x.numel()
    
    def forward(self, x):
        if x.dim() == 3:
            x = x.unsqueeze(1)
        
        x = self.block1(x)
        x = self.block2(x)
        features = x.view(x.size(0), -1)
        return features
    
    def predict(self, x):
        features = self.forward(x)
        logits = self.classifier(features)
        return logits
    
    def predict_proba(self, x):
        logits = self.predict(x)
        probs = torch.sigmoid(logits)
        return probs
    
    def classify(self, x):
        probs = self.predict_proba(x)
        return (probs > 0.5).float()

# ğŸ”¥ ä¸ºæ‰€æœ‰å…¶ä»–æ¨¡å‹ä¹Ÿæ·»åŠ ç›¸åŒçš„ä¿®å¤
class AttentionEEGNet(nn.Module):
    def __init__(self, n_channels=17, n_timepoints=250, n_classes=2, dropout=0.25):
        super().__init__()
        
        # ç½‘ç»œç»“æ„ä¿æŒä¸å˜...
        self.spatial_attention = nn.Sequential(
            nn.Conv1d(n_channels, n_channels // 4, 1),
            nn.ReLU(),
            nn.Conv1d(n_channels // 4, n_channels, 1),
            nn.Sigmoid()
        )
        
        self.temporal_conv = nn.Sequential(
            nn.Conv1d(n_channels, 64, 25, padding=12),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            
            nn.Conv1d(64, 128, 15, padding=7),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),
            
            nn.Conv1d(128, 256, 10, padding=4),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(16)
        )
        
        self.temporal_attention = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 256),
            nn.Sigmoid()
        )
        
        # ğŸ”¥ ä¿®å¤åˆ†ç±»å™¨ - ç¡®ä¿æœ€åä¸€å±‚æ˜¯Linear
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 16, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1)  # ğŸ”¥ è¾“å‡ºlogitsï¼Œä¸è¦sigmoid
        )
        
        # ğŸ”¥ å®‰å…¨çš„æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
        
        self.stratified = []
    
    def _initialize_weights(self):
        """å®‰å…¨çš„æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, (nn.Conv1d, nn.Conv2d)):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
        # ğŸ”¥ æœ€åä¸€å±‚ç‰¹æ®Šåˆå§‹åŒ–
        final_layer = None
        for layer in reversed(list(self.classifier.modules())):
            if isinstance(layer, nn.Linear):
                final_layer = layer
                break
        
        if final_layer is not None:
            with torch.no_grad():
                final_layer.bias.fill_(-0.5)
                final_layer.weight.normal_(0, 0.01)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­ - è¿”å›ç‰¹å¾"""
        if x.dim() == 4:
            x = x.squeeze(1)
        
        # ç©ºé—´æ³¨æ„åŠ›
        spatial_att = self.spatial_attention(x)
        x = x * spatial_att
        
        # æ—¶é—´å·ç§¯
        x = self.temporal_conv(x)  # (B, 256, 16)
        
        # æ—¶é—´æ³¨æ„åŠ›
        # å…¨å±€å¹³å‡æ± åŒ–è·å–æ—¶é—´ç‰¹å¾
        temporal_features = x.mean(dim=2)  # (B, 256)
        temporal_att = self.temporal_attention(temporal_features)  # (B, 256)
        temporal_att = temporal_att.unsqueeze(2)  # (B, 256, 1)
        
        # åº”ç”¨æ—¶é—´æ³¨æ„åŠ›
        x = x * temporal_att  # (B, 256, 16)
        
        # è¿”å›å±•å¹³çš„ç‰¹å¾
        features = x.view(x.size(0), -1)  # (B, 256*16)
        return features
    
    # ğŸ”¥ æ·»åŠ ç¼ºå¤±çš„æ–¹æ³•
    def predict(self, x):
        """é¢„æµ‹é˜¶æ®µï¼šè¿”å›logits"""
        features = self.forward(x)
        logits = self.classifier(features)
        return logits
    
    def predict_proba(self, x):
        """è¿”å›æ¦‚ç‡"""
        logits = self.predict(x)
        probs = torch.sigmoid(logits)
        return probs
    
    def classify(self, x):
        """åˆ†ç±»é˜¶æ®µï¼šè¿”å›ç±»åˆ«æ ‡ç­¾"""
        probs = self.predict_proba(x)
        return (probs > 0.5).float()

class ImprovedEEGNet(nn.Module):
    def __init__(self, n_channels=17, n_timepoints=250, n_classes=2, dropout=0.25):
        super().__init__()
        
        # ğŸ”¥ æ—¶é—´ç»´åº¦å·ç§¯ - æ•è·æ—¶é—´ç‰¹å¾
        self.temporal_conv = nn.Sequential(
            nn.Conv1d(n_channels, 32, kernel_size=25, padding=12),
            nn.BatchNorm1d(32),
            nn.ELU(),
            nn.MaxPool1d(2),
            nn.Dropout(dropout),
            
            nn.Conv1d(32, 64, kernel_size=15, padding=7),
            nn.BatchNorm1d(64),
            nn.ELU(),
            nn.MaxPool1d(2),
            nn.Dropout(dropout),
            
            nn.Conv1d(64, 128, kernel_size=10, padding=5),
            nn.BatchNorm1d(128),
            nn.ELU(),
            nn.MaxPool1d(2),
            nn.Dropout(dropout)
        )
        
        # ğŸ”¥ è®¡ç®—ç‰¹å¾ç»´åº¦
        self.feature_dim = 128 * (n_timepoints // 8)
        
        # ğŸ”¥ ä¿®å¤åˆ†ç±»å™¨ - ç§»é™¤Sigmoidï¼Œåªä¿ç•™Linear
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.feature_dim, 512),
            nn.ELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 128),
            nn.ELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1)  # ğŸ”¥ æœ€åä¸€å±‚æ˜¯Linearï¼Œè¾“å‡ºlogits
        )
        
        # ğŸ”¥ æ­£ç¡®çš„åˆå§‹åŒ–æ–¹å¼
        self._initialize_weights()
        
        self.stratified = []
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
        # ğŸ”¥ æœ€åä¸€å±‚ç‰¹æ®Šåˆå§‹åŒ–ï¼ˆè´Ÿåç½®ï¼‰
        if isinstance(self.classifier[-1], nn.Linear):
            with torch.no_grad():
                self.classifier[-1].bias.fill_(-0.5)  # ğŸ”¥ ç°åœ¨å¯ä»¥å®‰å…¨è®¿é—®bias
                self.classifier[-1].weight.normal_(0, 0.01)
    
    def forward(self, x):
        if x.dim() == 4:
            x = x.squeeze(1)
        
        # æ—¶é—´ç»´åº¦å·ç§¯
        x = self.temporal_conv(x)
        
        # è¿”å›ç‰¹å¾
        features = x.view(x.size(0), -1)
        return features
    
    def predict(self, x):
        features = self.forward(x)
        logits = self.classifier(features)  # ğŸ”¥ è¿”å›logits
        return logits
    
    def predict_proba(self, x):
        logits = self.predict(x)
        probs = torch.sigmoid(logits)  # ğŸ”¥ åœ¨è¿™é‡Œåº”ç”¨sigmoid
        return probs
    
    def classify(self, x):
        probs = self.predict_proba(x)
        return (probs > 0.5).float()

class AdvancedEEGNet(nn.Module):
    """æ›´é«˜çº§çš„EEGç½‘ç»œ - é›†æˆå¤šç§å…ˆè¿›æŠ€æœ¯"""
    def __init__(self, n_channels=17, n_timepoints=250, n_classes=2, dropout=0.2):
        super().__init__()
        
        # ğŸ”¥ å¤šå°ºåº¦æ—¶é—´å·ç§¯
        self.temporal_conv1 = nn.Sequential(
            nn.Conv1d(n_channels, 64, kernel_size=25, padding=12),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Dropout(dropout)
        )
        
        self.temporal_conv2 = nn.Sequential(
            nn.Conv1d(64, 128, kernel_size=15, padding=7),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Dropout(dropout)
        )
        
        self.temporal_conv3 = nn.Sequential(
            nn.Conv1d(128, 256, kernel_size=10, padding=5),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Dropout(dropout)
        )
        
        # ğŸ”¥ è‡ªé€‚åº”æ± åŒ–
        self.adaptive_pool = nn.AdaptiveAvgPool1d(32)
        
        # ğŸ”¥ åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 32, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1)  # ğŸ”¥ è¾“å‡ºlogits
        )
        
        # ğŸ”¥ æ·»åŠ æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
        
        self.stratified = []
    
    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–æ–¹æ³•"""
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
        # ğŸ”¥ æœ€åä¸€å±‚ç‰¹æ®Šåˆå§‹åŒ–
        if isinstance(self.classifier[-1], nn.Linear):
            with torch.no_grad():
                self.classifier[-1].bias.fill_(-0.5)
                self.classifier[-1].weight.normal_(0, 0.01)
        
        print("âœ… AdvancedEEGNet æƒé‡åˆå§‹åŒ–å®Œæˆ")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        if x.dim() == 4:
            x = x.squeeze(1)
        
        # å¤šå°ºåº¦æ—¶é—´å·ç§¯
        x = self.temporal_conv1(x)  # (B, 64, T/2)
        x = self.temporal_conv2(x)  # (B, 128, T/4)
        x = self.temporal_conv3(x)  # (B, 256, T/8)
        
        # è‡ªé€‚åº”æ± åŒ–
        x = self.adaptive_pool(x)   # (B, 256, 32)
        
        # å±•å¹³
        features = x.view(x.size(0), -1)  # (B, 256*32)
        return features
    
    def predict(self, x):
        """é¢„æµ‹logits"""
        features = self.forward(x)
        logits = self.classifier(features)
        return logits
    
    def predict_proba(self, x):
        """é¢„æµ‹æ¦‚ç‡"""
        logits = self.predict(x)
        probs = torch.sigmoid(logits)
        return probs
    
    def classify(self, x):
        """åˆ†ç±»é¢„æµ‹"""
        probs = self.predict_proba(x)
        return (probs > 0.5).float()

class ResidualEEGNet(nn.Module):
    """å¸¦æ®‹å·®è¿æ¥çš„EEGç½‘ç»œ"""
    def __init__(self, n_channels=17, n_timepoints=250, n_classes=2, dropout=0.1):
        super().__init__()
        
        # ğŸ”¥ è¾“å…¥æŠ•å½±
        self.input_proj = nn.Conv1d(n_channels, 64, 1)
        
        # ğŸ”¥ æ®‹å·®å—
        self.res_blocks = nn.ModuleList([
            self._make_res_block(64, 64, 7),
            self._make_res_block(64, 128, 5),
            self._make_res_block(128, 256, 3),
        ])
        
        # ğŸ”¥ æ³¨æ„åŠ›
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 256),
            nn.Sigmoid()
        )
        
        # ğŸ”¥ ä¿®å¤åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool1d(16),
            nn.Flatten(),
            nn.Linear(256 * 16, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1)  # ğŸ”¥ è¾“å‡ºlogits
        )
        
        self._initialize_weights()
        self.stratified = []
    
    def _make_res_block(self, in_channels, out_channels, kernel_size):
        """åˆ›å»ºæ®‹å·®å—"""
        return nn.Sequential(
            nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(),
            nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2),
            nn.BatchNorm1d(out_channels),
        )
    
    def forward(self, x):
        if x.dim() == 4:
            x = x.squeeze(1)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)
        
        # æ®‹å·®å—
        for i, block in enumerate(self.res_blocks):
            identity = x
            out = block(x)
            
            # è°ƒæ•´ç»´åº¦ç”¨äºæ®‹å·®è¿æ¥
            if identity.shape[1] != out.shape[1]:
                identity = F.interpolate(identity, size=out.shape[2])
                identity = nn.Conv1d(identity.shape[1], out.shape[1], 1).to(x.device)(identity)
            
            x = F.relu(out + identity)
            x = F.max_pool1d(x, 2)
        
        # æ³¨æ„åŠ›
        att_weights = self.attention(x).unsqueeze(-1)
        x = x * att_weights
        
        return x.view(x.size(0), -1)
    
    def predict(self, x):
        features = self.forward(x)
        logits = self.classifier(features)
        return logits

def create_binary_model(cfg):
    """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹"""
    model_type = getattr(cfg.model, 'model_type', 'improved_eegnet')
    
    if model_type == 'improved_eegnet':
        model = ImprovedEEGNet(
            n_channels=cfg.model.n_channels,
            n_timepoints=cfg.model.n_timepoints,
            n_classes=cfg.model.n_classes,
            dropout=cfg.model.dropout
        )
    elif model_type == 'advanced':
        model = AdvancedEEGNet(
            n_channels=cfg.model.n_channels,
            n_timepoints=cfg.model.n_timepoints,
            n_classes=cfg.model.n_classes,
            dropout=cfg.model.dropout
        )
    elif model_type == 'residual_eegnet':
        model = ResidualEEGNet(
            n_channels=cfg.model.n_channels,
            n_timepoints=cfg.model.n_timepoints,
            n_classes=cfg.model.n_classes,
            dropout=cfg.model.dropout
        )
    elif model_type == 'eegnet':
        model = EEGNet(
            n_channels=cfg.model.n_channels,
            n_timepoints=cfg.model.n_timepoints,
            n_classes=cfg.model.n_classes,
            dropout=cfg.model.dropout
        )
    elif model_type == 'attention':
        model = AttentionEEGNet(
            n_channels=cfg.model.n_channels,
            n_timepoints=cfg.model.n_timepoints,
            n_classes=cfg.model.n_classes,
            dropout=cfg.model.dropout
        )
    else:
        # åŸå§‹æ¨¡å‹
        model = EEGBinaryModel(
            n_channels=cfg.model.n_channels,
            n_timepoints=cfg.model.n_timepoints,
            d_model=cfg.model.d_model,
            n_classes=cfg.model.n_classes,
            dropout=cfg.model.dropout
        )
    
    return model