import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score
import numpy as np

class BinaryExtractorModel(pl.LightningModule):
    """äºŒåˆ†ç±»ç‰¹å¾æå–å™¨æ¨¡å‹ - ä¿®å¤ä¸ºBCEWithLogitsLoss"""
    
    def __init__(self, model, cfg_train):
        super().__init__()
        self.model = model
        self.cfg_train = cfg_train
        
        # ğŸ”¥ ä½¿ç”¨BCEWithLogitsLossï¼Œå¯¹æ··åˆç²¾åº¦å®‰å…¨
        self.criterion = nn.BCEWithLogitsLoss()
        
        # ğŸ”¥ ç”¨äºæ”¶é›†éªŒè¯ç»“æœ
        self.validation_step_outputs = []
        
        # ğŸ”¥ æ‰‹åŠ¨è·Ÿè¸ªæœ€ä½³æŒ‡æ ‡
        self.best_val_acc = 0.0
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.best_metrics = {}
        
    def forward(self, x):
        return self.model.predict(x)  # è¿”å›logits
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        
        # ğŸ”¥ å¢å¼ºæ•°æ®å¢å¼º
        if self.training:
            x = self.add_noise(x, noise_factor=0.1)  # ğŸ”¥ å¢åŠ å™ªå£°
    
        logits = self.forward(x).squeeze(1)
        y_float = y.float()
        
        # ğŸ”¥ æ·»åŠ L2æ­£åˆ™åŒ–åˆ°æŸå¤±
        l2_reg = torch.tensor(0.0, device=self.device)
        for param in self.model.parameters():
            l2_reg += torch.norm(param, p=2)
    
        base_loss = self.criterion(logits, y_float)
        loss = base_loss + 0.01 * l2_reg  # ğŸ”¥ æ·»åŠ L2æƒ©ç½š
    
        # è®¡ç®—å‡†ç¡®ç‡
        probs = torch.sigmoid(logits)
        pred_labels = (probs > 0.5).float()
        acc = (pred_labels == y_float).float().mean()
    
        self.log_dict({
            'ext/train/loss': loss,
            'ext/train/base_loss': base_loss,
            # 'ext/train/l2_reg': l2_reg,
            'ext/train/acc': acc,
        }, on_step=False, on_epoch=True, prog_bar=True)
    
        return loss
    
    def validation_step(self, batch, batch_idx):
        x, y = batch
        
        # ğŸ”¥ è·å–logits
        logits = self.forward(x)
        logits = logits.squeeze(1)
        
        # ğŸ”¥ æ ‡ç­¾è½¬æ¢
        y_float = y.float()
        
        # ğŸ”¥ è®¡ç®—æŸå¤±
        loss = self.criterion(logits, y_float)
        
        # ğŸ”¥ è®¡ç®—å‡†ç¡®ç‡
        probs = torch.sigmoid(logits)
        pred_labels = (probs > 0.5).float()
        acc = (pred_labels == y_float).float().mean()
        
        # ğŸ”¥ è®°å½•ç»“æœ
        self.validation_step_outputs.append({
            'val_loss': loss,
            'val_acc': acc,
            'predictions': pred_labels,
            'targets': y_float,
            'probabilities': probs,
            'logits': logits
        })
        
        return loss
    
    def on_validation_epoch_end(self):
        if len(self.validation_step_outputs) == 0:
            return
        
        # ğŸ”¥ æ”¶é›†æ‰€æœ‰é¢„æµ‹å’Œç›®æ ‡
        all_preds = torch.cat([x['predictions'] for x in self.validation_step_outputs])
        all_targets = torch.cat([x['targets'] for x in self.validation_step_outputs])
        all_probs = torch.cat([x['probabilities'] for x in self.validation_step_outputs])
        all_logits = torch.cat([x['logits'] for x in self.validation_step_outputs])
        
        # ğŸ”¥ å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        from sklearn.metrics import precision_recall_curve, roc_curve
        
        targets_np = all_targets.cpu().numpy()
        probs_np = all_probs.cpu().numpy()
        
        # æ–¹æ³•1ï¼šä½¿ç”¨F1åˆ†æ•°å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        thresholds = np.linspace(0.1, 0.9, 50)
        best_threshold = 0.55
        best_f1 = 0
        
        for threshold in thresholds:
            pred_at_thresh = (probs_np > threshold).astype(float)
            if len(np.unique(pred_at_thresh)) > 1:  # ç¡®ä¿æœ‰ä¸¤ä¸ªç±»åˆ«
                f1 = f1_score(targets_np, pred_at_thresh, zero_division=0)
                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = threshold
        
        # ğŸ”¥ ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼é‡æ–°è®¡ç®—æŒ‡æ ‡
        optimal_preds = (probs_np > best_threshold).astype(float)
        
        precision = precision_score(targets_np, optimal_preds, average='binary', zero_division=0)
        recall = recall_score(targets_np, optimal_preds, average='binary', zero_division=0)
        f1 = f1_score(targets_np, optimal_preds, average='binary', zero_division=0)
        acc = accuracy_score(targets_np, optimal_preds)
        
        # ğŸ”¥ è®¡ç®—AUC
        try:
            auc = roc_auc_score(targets_np, probs_np) if len(np.unique(targets_np)) > 1 else 0.0
        except:
            auc = 0.0
        
        # ğŸ”¥ è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = torch.stack([x['val_loss'] for x in self.validation_step_outputs]).mean()
        
        # ğŸ”¥ è®°å½•æŒ‡æ ‡ï¼ˆä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ï¼‰
        self.log_dict({
            'ext/val/loss': avg_loss,
            'ext/val/acc': acc,
            'ext/val/acc_optimal': acc,
            # 'ext/val/precision': precision,
            # 'ext/val/recall': recall,
            # 'ext/val/f1': f1,
            # 'ext/val/auc': auc,
            'ext/val/optimal_threshold': best_threshold,
        }, on_epoch=True, prog_bar=True)
        
        # ğŸ”¥ æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š éªŒè¯ç»“æœ (é˜ˆå€¼ä¼˜åŒ–):")
        print(f"   æœ€ä¼˜é˜ˆå€¼: {best_threshold:.3f}ï¼Œå‡†ç¡®ç‡: {acc:.4f}")
        # print(f"   ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"   å¬å›ç‡: {recall:.4f}ï¼Œå¹³å‡æ¦‚ç‡: {all_probs.mean():.4f}")
        # print(f"   F1åˆ†æ•°: {f1:.4f}")
        # print(f"   AUC: {auc:.4f}")
        # print(f"   å¹³å‡logits: {all_logits.mean():.4f}")
        print(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {targets_np.mean():.4f}")
        print(f"   é¢„æµ‹æ­£æ ·æœ¬æ¯”ä¾‹: {optimal_preds.mean():.4f}")
        
        # ğŸ”¥ æ¦‚ç‡åˆ†å¸ƒåˆ†æ
        pos_probs = probs_np[targets_np == 1]
        neg_probs = probs_np[targets_np == 0]
        print(f"   æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {pos_probs.mean():.4f} Â± {pos_probs.std():.4f}")
        print(f"   è´Ÿæ ·æœ¬å¹³å‡æ¦‚ç‡: {neg_probs.mean():.4f} Â± {neg_probs.std():.4f}")
        
        # ğŸ”¥ æ‰‹åŠ¨è®°å½•æœ€ä½³æŒ‡æ ‡
        if acc > self.best_val_acc:
            self.best_val_acc = float(acc)
            self.best_val_loss = float(avg_loss)
            self.best_epoch = self.current_epoch
            self.best_metrics = {
                'val_acc': float(acc),
                'val_loss': float(avg_loss),
                'val_recall': float(recall),
                'optimal_threshold': float(best_threshold),
                'epoch': self.current_epoch
            }
            
            print(f"ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯ç»“æœ! Epoch {self.current_epoch + 1}")
            print(f"   éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
            print(f"   éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            print(f"   æœ€ä¼˜é˜ˆå€¼: {self.best_metrics['optimal_threshold']:.4f}")
        
        # æ¸…ç©ºç»“æœ
        self.validation_step_outputs.clear()
    
    def configure_optimizers(self):
        # ğŸ”¥ å¤§å¹…é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ æ­£åˆ™åŒ–
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.cfg_train.lr * 0.1,  # ğŸ”¥ é™ä½å­¦ä¹ ç‡åˆ°1/10
            weight_decay=self.cfg_train.weight_decay * 5,  # ğŸ”¥ å¢åŠ æƒé‡è¡°å‡
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # ğŸ”¥ ä½¿ç”¨æ›´ä¿å®ˆçš„è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer, 
            step_size=3,  # æ¯3ä¸ªepoché™ä½å­¦ä¹ ç‡
            gamma=0.7     # ä¹˜ä»¥0.7
        )
        
        return [optimizer], [scheduler]
    
    def add_noise(self, x, noise_factor=0.1):
        """å¢å¼ºæ•°æ®å¢å¼º"""
        if self.training:
            # é«˜æ–¯å™ªå£°
            noise = torch.randn_like(x) * noise_factor
            x = x + noise
            
            # ğŸ”¥ éšæœºmaskä¸€äº›é€šé“
            if torch.rand(1).item() < 0.3:  # 30%æ¦‚ç‡
                n_channels = x.size(1)
                mask_channels = torch.randperm(n_channels)[:n_channels//4]  # mask 1/4é€šé“
                x[:, mask_channels, :] *= 0.5
            
            return x
        return x

# ä¿æŒä¸ä¸»é¡¹ç›®ä¸€è‡´çš„å‘½å
ExtractorModel = BinaryExtractorModel